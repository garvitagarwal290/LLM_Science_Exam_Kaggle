{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install peft\n!pip install datasets==2.15\n!pip install faiss-gpu # Use faiss-gpu if on GPU machine (faster)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport pandas as pd\nfrom datasets import load_dataset, Dataset, load_from_disk\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForMultipleChoice, BertForMultipleChoice, TrainingArguments, Trainer, AutoModelForSequenceClassification\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n\ndevice = get_default_device()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the dataset\nfiles = list(map(str, Path(\"/kaggle/input/wiki-20220301-en-sci\").glob(\"*.parquet\")))\nds = load_dataset(\"parquet\", data_files=files, split=\"train\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean(x):\n    x = x.split('References\\n')[0]\n    x = x.split('References \\n')[0]\n    x = x.split('See also\\n')[0]\n    x = x.split('See also \\n')[0]\n    x = x.split('External links')[0]\n    return x\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_dataset = Dataset.from_dict({\n    \"title\":ds['title'],\n    \"text\":[clean(t) for t in ds['text']],\n})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\nret_tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nret_model = AutoModel.from_pretrained(model_ckpt)\nret_model.to(device).eval()\n\n# Getting the final embedding from the model\ndef cls_pooling(model_output):\n    return model_output.last_hidden_state[:, 0]\n\ndef get_ret_embeddings(text_list):\n    encoded_input = ret_tokenizer(\n        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n    )\n    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n    model_output = ret_model(**encoded_input)\n    return cls_pooling(model_output)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#unused code for dividing the articles into chunks before converting into embeddings\n\n# chunked_text = []\n# for i in range(100):#len(clean_dataset['text'])):\n# #     if(i%100==0):print(i)\n#     print(i)    \n    \n#     encoded_input = ret_tokenizer(\n#         clean_dataset['text'][i], padding=True, truncation=False, return_tensors=\"pt\"\n#     )\n#     num_tokens = len(encoded_input['input_ids'][0])\n    \n#     chunksize = int(450/num_tokens * len(clean_dataset['text'][i]))\n\n#     frac = num_tokens/450 - num_tokens//450\n\n#     if(num_tokens <= 450*1.5): num_chunks = 1\n#     else:\n#         if(frac > 0.5): \n#             num_chunks = num_tokens//450 + 1\n\n#         else:\n#             num_chunks = num_tokens//450\n        \n#     for j in range(num_chunks):\n#         if(j+1 < num_chunks): chunked_text.append(clean_dataset['text'][i][j*chunksize:(j+1)*chunksize])\n#         else: chunked_text.append(clean_dataset['text'][i][(num_chunks-1)*chunksize : ])\n            \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ret_embeddings_dataset = clean_dataset.map(\n    lambda x: {\"embeddings\": get_ret_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ret_embeddings_dataset.save_to_disk('/kaggle/working/retrieval-wiki-embeddings')","metadata":{},"execution_count":null,"outputs":[]}]}